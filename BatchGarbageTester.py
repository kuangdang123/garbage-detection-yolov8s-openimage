import os
import time
import json
import pandas as pd
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from ultralytics import YOLO
from pathlib import Path
import cv2
from PIL import Image

class BatchGarbageTester:
    def __init__(self, model_path, test_images_dir, output_dir="batch_test_results"):
        """
        æ‰¹é‡åƒåœ¾æ£€æµ‹æµ‹è¯•ç±»
    
        model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        test_images_dir: æµ‹è¯•å›¾ç‰‡ç›®å½•
        output_dir: è¾“å‡ºç»“æœç›®å½•
        """
        self.model = YOLO(model_path)
        self.test_images_dir = Path(test_images_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "detection_results").mkdir(exist_ok=True)
        (self.output_dir / "visualizations").mkdir(exist_ok=True)
        (self.output_dir / "metrics").mkdir(exist_ok=True)
        
        # åƒåœ¾åˆ†ç±»æ˜ å°„
        self.garbage_categories = {
            "Bottle": "å¯å›æ”¶ç‰©",
            "Book": "å¯å›æ”¶ç‰©",
            "Mobile phone": "æœ‰å®³åƒåœ¾", 
            "Banana": "å¨ä½™åƒåœ¾",
            "Apple": "å¨ä½™åƒåœ¾",
            "Orange": "å¨ä½™åƒåœ¾",
            "Plastic bag": "å…¶ä»–åƒåœ¾",
            "Toilet paper": "å…¶ä»–åƒåœ¾",
            "Coffee cup": "å…¶ä»–åƒåœ¾"
        }
        
        # å­˜å‚¨æµ‹è¯•ç»“æœ
        self.results = []
        self.detection_stats = {
            "total_images": 0,
            "total_detections": 0,
            "category_counts": {},
            "confidence_stats": [],
            "inference_times": []
        }
        
    def run_batch_test(self, confidence_threshold=0.5, iou_threshold=0.5, max_images=None):
        """
        è¿è¡Œæ‰¹é‡æµ‹è¯•
        
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        iou_threshold: NMS IoUé˜ˆå€¼
        max_images: æœ€å¤§æµ‹è¯•å›¾ç‰‡æ•°é‡ï¼ˆNoneè¡¨ç¤ºæµ‹è¯•æ‰€æœ‰ï¼‰
        """
        print("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        
        # è·å–æµ‹è¯•å›¾ç‰‡
        image_paths = list(self.test_images_dir.glob("*.jpg")) + \
                     list(self.test_images_dir.glob("*.png")) + \
                     list(self.test_images_dir.glob("*.jpeg"))
        
        if max_images:
            image_paths = image_paths[:max_images]
        
        self.detection_stats["total_images"] = len(image_paths)
        
        # è¿›åº¦æ¡
        progress_bar = tqdm(image_paths, desc="æµ‹è¯•è¿›åº¦")
        
        for image_path in progress_bar:
            try:
                # è®°å½•å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # æ‰§è¡Œæ£€æµ‹
                results = self.model.predict(
                    source=str(image_path),
                    conf=confidence_threshold,
                    iou=iou_threshold,
                    max_det=100,
                    verbose=False
                )
                
                # è®°å½•æ¨ç†æ—¶é—´
                inference_time = time.time() - start_time
                self.detection_stats["inference_times"].append(inference_time)
                
                # å¤„ç†æ£€æµ‹ç»“æœ
                image_results = self._process_single_image_results(
                    results, str(image_path), inference_time
                )
                
                self.results.append(image_results)
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                progress_bar.set_postfix({
                    'å·²æ£€æµ‹': len(self.results),
                    'å¹³å‡æ—¶é—´': f"{np.mean(self.detection_stats['inference_times']):.3f}s"
                })
                
            except Exception as e:
                print(f"âŒ å¤„ç†å›¾ç‰‡ {image_path} æ—¶å‡ºé”™: {e}")
                continue
        
        print("âœ… æ‰¹é‡æµ‹è¯•å®Œæˆ!")
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_test_report(confidence_threshold, iou_threshold)
        
        return self.results
    
    def _process_single_image_results(self, results, image_path, inference_time):
        """å¤„ç†å•å¼ å›¾ç‰‡çš„æ£€æµ‹ç»“æœ"""
        image_results = {
            "image_path": image_path,
            "image_name": Path(image_path).name,
            "inference_time": inference_time,
            "detections": [],
            "detection_count": 0,
            "categories_found": set()
        }
        
        for result in results:
            if result.boxes is not None:
                for box in result.boxes:
                    class_id = int(box.cls[0])
                    class_name = self.model.names[class_id]
                    confidence = float(box.conf[0])
                    bbox = box.xyxy[0].tolist()
                    
                    # è·å–åƒåœ¾åˆ†ç±»
                    garbage_category = self.garbage_categories.get(class_name, "æœªçŸ¥åˆ†ç±»")
                    
                    detection_info = {
                        "class_name": class_name,
                        "category": garbage_category,
                        "confidence": confidence,
                        "bbox": bbox,
                        "area": (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
                    }
                    
                    image_results["detections"].append(detection_info)
                    image_results["categories_found"].add(garbage_category)
                    
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    self.detection_stats["total_detections"] += 1
                    self.detection_stats["category_counts"][garbage_category] = \
                        self.detection_stats["category_counts"].get(garbage_category, 0) + 1
                    self.detection_stats["confidence_stats"].append(confidence)
        
        image_results["detection_count"] = len(image_results["detections"])
        image_results["categories_found"] = list(image_results["categories_found"])
        
        return image_results
    
    def _generate_test_report(self, confidence_threshold, iou_threshold):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        # 1. ä¿å­˜åŸå§‹ç»“æœ
        self._save_raw_results()
        
        # 2. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_statistical_report(confidence_threshold, iou_threshold)
        
        # 3. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_visualizations()
        
        # 4. ç”ŸæˆYOLOæ ¼å¼çš„è¯„ä¼°æŒ‡æ ‡
        self._generate_yolo_metrics()
    
    def _save_raw_results(self):
        """ä¿å­˜åŸå§‹æ£€æµ‹ç»“æœ"""
        # ä¿å­˜ä¸ºJSON
        with open(self.output_dir / "raw_detection_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸ºCSV
        csv_data = []
        for result in self.results:
            for detection in result["detections"]:
                csv_data.append({
                    "image_path": result["image_path"],
                    "image_name": result["image_name"],
                    "class_name": detection["class_name"],
                    "category": detection["category"],
                    "confidence": detection["confidence"],
                    "inference_time": result["inference_time"]
                })
        
        df = pd.DataFrame(csv_data)
        df.to_csv(self.output_dir / "detection_results.csv", index=False, encoding="utf-8")
    
    def _generate_statistical_report(self, confidence_threshold, iou_threshold):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        stats = self.detection_stats
        
        # åŸºæœ¬ç»Ÿè®¡
        report = {
            "æµ‹è¯•é…ç½®": {
                "æ¨¡å‹è·¯å¾„": str(self.model.ckpt_path),
                "æµ‹è¯•å›¾ç‰‡æ•°é‡": stats["total_images"],
                "ç½®ä¿¡åº¦é˜ˆå€¼": confidence_threshold,
                "IoUé˜ˆå€¼": iou_threshold,
                "æ€»æ£€æµ‹æ•°é‡": stats["total_detections"]
            },
            "æ€§èƒ½ç»Ÿè®¡": {
                "å¹³å‡æ¨ç†æ—¶é—´": f"{np.mean(stats['inference_times']):.4f}ç§’",
                "æœ€å¿«æ¨ç†æ—¶é—´": f"{np.min(stats['inference_times']):.4f}ç§’",
                "æœ€æ…¢æ¨ç†æ—¶é—´": f"{np.max(stats['inference_times']):.4f}ç§’",
                "FPS": f"{1/np.mean(stats['inference_times']):.2f}",
                "æ€»æµ‹è¯•æ—¶é—´": f"{np.sum(stats['inference_times']):.2f}ç§’"
            },
            "æ£€æµ‹ç»Ÿè®¡": {
                "å¹³å‡æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°": f"{stats['total_detections'] / stats['total_images']:.2f}",
                "æ£€æµ‹ç‡": f"{(sum(1 for r in self.results if r['detection_count'] > 0) / stats['total_images']) * 100:.2f}%",
                "å¹³å‡ç½®ä¿¡åº¦": f"{np.mean(stats['confidence_stats']):.4f}",
                "ç½®ä¿¡åº¦æ ‡å‡†å·®": f"{np.std(stats['confidence_stats']):.4f}"
            },
            "åˆ†ç±»ç»Ÿè®¡": stats["category_counts"]
        }
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        with open(self.output_dir / "statistical_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(report)
    
    def _generate_markdown_report(self, report):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        md_content = f"""# åƒåœ¾æ£€æµ‹æ‰¹é‡æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•é…ç½®
- **æ¨¡å‹**: {report['æµ‹è¯•é…ç½®']['æ¨¡å‹è·¯å¾„']}
- **æµ‹è¯•å›¾ç‰‡æ•°é‡**: {report['æµ‹è¯•é…ç½®']['æµ‹è¯•å›¾ç‰‡æ•°é‡']}
- **ç½®ä¿¡åº¦é˜ˆå€¼**: {report['æµ‹è¯•é…ç½®']['ç½®ä¿¡åº¦é˜ˆå€¼']}
- **IoUé˜ˆå€¼**: {report['æµ‹è¯•é…ç½®']['IoUé˜ˆå€¼']}
- **æ€»æ£€æµ‹æ•°é‡**: {report['æµ‹è¯•é…ç½®']['æ€»æ£€æµ‹æ•°é‡']}

## æ€§èƒ½ç»Ÿè®¡
- **å¹³å‡æ¨ç†æ—¶é—´**: {report['æ€§èƒ½ç»Ÿè®¡']['å¹³å‡æ¨ç†æ—¶é—´']}
- **FPS**: {report['æ€§èƒ½ç»Ÿè®¡']['FPS']}
- **æ€»æµ‹è¯•æ—¶é—´**: {report['æ€§èƒ½ç»Ÿè®¡']['æ€»æµ‹è¯•æ—¶é—´']}

## æ£€æµ‹ç»Ÿè®¡
- **å¹³å‡æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°**: {report['æ£€æµ‹ç»Ÿè®¡']['å¹³å‡æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°']}
- **æ£€æµ‹ç‡**: {report['æ£€æµ‹ç»Ÿè®¡']['æ£€æµ‹ç‡']}
- **å¹³å‡ç½®ä¿¡åº¦**: {report['æ£€æµ‹ç»Ÿè®¡']['å¹³å‡ç½®ä¿¡åº¦']}

## åƒåœ¾åˆ†ç±»ç»Ÿè®¡
"""
        
        for category, count in report['åˆ†ç±»ç»Ÿè®¡'].items():
            percentage = (count / report['æµ‹è¯•é…ç½®']['æ€»æ£€æµ‹æ•°é‡']) * 100
            md_content += f"- **{category}**: {count} ä¸ª ({percentage:.1f}%)\\n"
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        with open(self.output_dir / "test_report.md", "w", encoding="utf-8") as f:
            f.write(md_content)
    
    def _generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. åƒåœ¾åˆ†ç±»åˆ†å¸ƒé¥¼å›¾
        self._plot_category_distribution()
        
        # 2. æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
        self._plot_confidence_distribution()
        
        # 3. æ¨ç†æ—¶é—´åˆ†å¸ƒå›¾
        self._plot_inference_time_distribution()
        
        # 4. æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°é‡åˆ†å¸ƒ
        self._plot_detections_per_image()
        
        # 5. å„ç±»åˆ«æ£€æµ‹æ•°é‡æŸ±çŠ¶å›¾
        self._plot_category_bar_chart()
    
    def _plot_category_distribution(self):
        """ç»˜åˆ¶åƒåœ¾åˆ†ç±»åˆ†å¸ƒé¥¼å›¾"""
        category_counts = self.detection_stats["category_counts"]
        
        if not category_counts:
            return
        
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = ['#4CAF50', '#F44336', '#FF9800', '#9E9E9E', '#2196F3']
        
        wedges, texts, autotexts = ax.pie(
            category_counts.values(),
            labels=category_counts.keys(),
            autopct='%1.1f%%',
            startangle=90,
            colors=colors[:len(category_counts)]
        )
        
        # ç¾åŒ–æ–‡æœ¬
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
        
        ax.set_title('åƒåœ¾åˆ†ç±»åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "category_distribution.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_confidence_distribution(self):
        """ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾"""
        if not self.detection_stats["confidence_stats"]:
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.hist(self.detection_stats["confidence_stats"], bins=20, 
               alpha=0.7, color='skyblue', edgecolor='black')
        
        ax.set_xlabel('ç½®ä¿¡åº¦')
        ax.set_ylabel('é¢‘ç‡')
        ax.set_title('æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_conf = np.mean(self.detection_stats["confidence_stats"])
        ax.axvline(mean_conf, color='red', linestyle='--', 
                  label=f'å¹³å‡ç½®ä¿¡åº¦: {mean_conf:.3f}')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "confidence_distribution.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_inference_time_distribution(self):
        """ç»˜åˆ¶æ¨ç†æ—¶é—´åˆ†å¸ƒå›¾"""
        if not self.detection_stats["inference_times"]:
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.hist(self.detection_stats["inference_times"], bins=20, 
               alpha=0.7, color='lightgreen', edgecolor='black')
        
        ax.set_xlabel('æ¨ç†æ—¶é—´ (ç§’)')
        ax.set_ylabel('é¢‘ç‡')
        ax.set_title('æ¨ç†æ—¶é—´åˆ†å¸ƒ')
        ax.grid(True, alpha=0.3)
        
        mean_time = np.mean(self.detection_stats["inference_times"])
        ax.axvline(mean_time, color='red', linestyle='--', 
                  label=f'å¹³å‡æ—¶é—´: {mean_time:.3f}s')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "inference_time_distribution.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_detections_per_image(self):
        """ç»˜åˆ¶æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°é‡åˆ†å¸ƒ"""
        detections_per_image = [r["detection_count"] for r in self.results]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.hist(detections_per_image, bins=20, alpha=0.7, 
               color='orange', edgecolor='black')
        
        ax.set_xlabel('æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°é‡')
        ax.set_ylabel('å›¾ç‰‡æ•°é‡')
        ax.set_title('æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°é‡åˆ†å¸ƒ')
        ax.grid(True, alpha=0.3)
        
        mean_detections = np.mean(detections_per_image)
        ax.axvline(mean_detections, color='red', linestyle='--', 
                  label=f'å¹³å‡æ£€æµ‹æ•°: {mean_detections:.2f}')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "detections_per_image.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_category_bar_chart(self):
        """ç»˜åˆ¶å„ç±»åˆ«æ£€æµ‹æ•°é‡æŸ±çŠ¶å›¾"""
        category_counts = self.detection_stats["category_counts"]
        
        if not category_counts:
            return
        
        fig, ax = plt.subplots(figsize=(12, 6))
        categories = list(category_counts.keys())
        counts = list(category_counts.values())
        
        bars = ax.bar(categories, counts, color=['#4CAF50', '#F44336', '#FF9800', '#9E9E9E'])
        
        ax.set_xlabel('åƒåœ¾ç±»åˆ«')
        ax.set_ylabel('æ£€æµ‹æ•°é‡')
        ax.set_title('å„ç±»åˆ«æ£€æµ‹æ•°é‡')
        ax.tick_params(axis='x', rotation=45)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{count}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "category_bar_chart.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_yolo_metrics(self):
        """ç”ŸæˆYOLOæ ¼å¼çš„è¯„ä¼°æŒ‡æ ‡"""
        # å¦‚æœæœ‰æ ‡æ³¨æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨YOLOçš„valæ–¹æ³•
        # è¿™é‡Œæˆ‘ä»¬ç”Ÿæˆä¸€äº›åŸºäºæ£€æµ‹ç»“æœçš„è¡ç”ŸæŒ‡æ ‡
        
        metrics = {
            "æ£€æµ‹è¦†ç›–ç‡": self._calculate_detection_coverage(),
            "ç±»åˆ«å¹³è¡¡åº¦": self._calculate_category_balance(),
            "æ£€æµ‹ç¨³å®šæ€§": self._calculate_detection_stability(),
            "æ€§èƒ½æŒ‡æ ‡": {
                "å¹³å‡FPS": 1 / np.mean(self.detection_stats["inference_times"]),
                "ååé‡": len(self.results) / np.sum(self.detection_stats["inference_times"]),
                "æ£€æµ‹å¯†åº¦": self.detection_stats["total_detections"] / self.detection_stats["total_images"]
            }
        }
        
        with open(self.output_dir / "metrics" / "yolo_style_metrics.json", "w") as f:
            json.dump(metrics, f, indent=2)
    
    def _calculate_detection_coverage(self):
        """è®¡ç®—æ£€æµ‹è¦†ç›–ç‡"""
        images_with_detections = sum(1 for r in self.results if r["detection_count"] > 0)
        return images_with_detections / self.detection_stats["total_images"]
    
    def _calculate_category_balance(self):
        """è®¡ç®—ç±»åˆ«å¹³è¡¡åº¦"""
        category_counts = list(self.detection_stats["category_counts"].values())
        if not category_counts:
            return 0
        return min(category_counts) / max(category_counts)
    
    def _calculate_detection_stability(self):
        """è®¡ç®—æ£€æµ‹ç¨³å®šæ€§ï¼ˆæ£€æµ‹æ•°é‡çš„å˜å¼‚ç³»æ•°ï¼‰"""
        detection_counts = [r["detection_count"] for r in self.results]
        if not detection_counts:
            return 0
        return np.std(detection_counts) / np.mean(detection_counts)
    
    def get_summary(self):
        """è·å–æµ‹è¯•æ‘˜è¦"""
        return {
            "total_images": self.detection_stats["total_images"],
            "total_detections": self.detection_stats["total_detections"],
            "avg_inference_time": np.mean(self.detection_stats["inference_times"]),
            "avg_confidence": np.mean(self.detection_stats["confidence_stats"]),
            "category_breakdown": self.detection_stats["category_counts"]
        }