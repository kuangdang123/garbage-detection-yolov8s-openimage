import os
import time
import json
import pandas as pd
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from ultralytics import YOLO
from pathlib import Path
import cv2
from PIL import Image
import yaml

class BatchGarbageTester:
    def __init__(self, model_path, test_images_dir, output_dir="batch_test_results"):
        """
        æ‰¹é‡åƒåœ¾æ£€æµ‹æµ‹è¯•ç±»
    
        model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        test_images_dir: æµ‹è¯•å›¾ç‰‡ç›®å½•
        output_dir: è¾“å‡ºç»“æœç›®å½•
        """
        self.model = YOLO(model_path)
        self.test_images_dir = Path(test_images_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "detection_results").mkdir(exist_ok=True)
        (self.output_dir / "visualizations").mkdir(exist_ok=True)
        (self.output_dir / "metrics").mkdir(exist_ok=True)

        # åˆ›å»ºYOLOæ ¼å¼æ ‡æ³¨ç›®å½•
        (self.output_dir / "yolo_labels").mkdir(exist_ok=True)
        (self.output_dir / "yolo_format").mkdir(exist_ok=True)
        
        # åˆ›å»ºè¯„ä¼°ç»“æœç›®å½•
        (self.output_dir / "evaluation").mkdir(exist_ok=True)
        (self.output_dir / "evaluation" / "plots").mkdir(exist_ok=True)
        
        # åƒåœ¾åˆ†ç±»æ˜ å°„
        self.garbage_categories = {
            "Bottle": "å¯å›æ”¶ç‰©",
            "Book": "å¯å›æ”¶ç‰©",
            "Mobile phone": "æœ‰å®³åƒåœ¾", 
            "Banana": "å¨ä½™åƒåœ¾",
            "Apple": "å¨ä½™åƒåœ¾",
            "Orange": "å¨ä½™åƒåœ¾",
            "Plastic bag": "å…¶ä»–åƒåœ¾",
            "Toilet paper": "å…¶ä»–åƒåœ¾",
            "Coffee cup": "å…¶ä»–åƒåœ¾"
        }
        
        # å­˜å‚¨æµ‹è¯•ç»“æœ
        self.results = []
        self.detection_stats = {
            "total_images": 0,
            "total_detections": 0,
            "category_counts": {},
            "confidence_stats": [],
            "inference_times": []
        }

        # å­˜å‚¨ç±»åˆ«IDæ˜ å°„
        self.class_to_id = {}
        self._setup_class_mapping()

    def _setup_class_mapping(self):
        """è®¾ç½®ç±»åˆ«åç§°åˆ°IDçš„æ˜ å°„"""
        unique_classes = set(self.garbage_categories.keys())
        self.class_to_id = {cls: idx for idx, cls in enumerate(sorted(unique_classes))}
        
    def run_batch_test(self, confidence_threshold=0.5, iou_threshold=0.5, max_images=None):
        """
        è¿è¡Œæ‰¹é‡æµ‹è¯•
        
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        iou_threshold: NMS IoUé˜ˆå€¼
        max_images: æœ€å¤§æµ‹è¯•å›¾ç‰‡æ•°é‡ï¼ˆNoneè¡¨ç¤ºæµ‹è¯•æ‰€æœ‰ï¼‰
        """
        print("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        
        # è·å–æµ‹è¯•å›¾ç‰‡
        image_paths = list(self.test_images_dir.glob("*.jpg")) + \
                     list(self.test_images_dir.glob("*.png")) + \
                     list(self.test_images_dir.glob("*.jpeg"))
        
        if max_images:
            image_paths = image_paths[:max_images]
        
        self.detection_stats["total_images"] = len(image_paths)
        
        # è¿›åº¦æ¡
        progress_bar = tqdm(image_paths, desc="æµ‹è¯•è¿›åº¦")
        
        for image_path in progress_bar:
            try:
                # è®°å½•å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # æ‰§è¡Œæ£€æµ‹
                results = self.model.predict(
                    source=str(image_path),
                    conf=confidence_threshold,
                    iou=iou_threshold,
                    max_det=100,
                    verbose=False
                )
                
                # è®°å½•æ¨ç†æ—¶é—´
                inference_time = time.time() - start_time
                self.detection_stats["inference_times"].append(inference_time)
                
                # å¤„ç†æ£€æµ‹ç»“æœ
                image_results = self._process_single_image_results(
                    results, str(image_path), inference_time
                )
                
                self.results.append(image_results)
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                progress_bar.set_postfix({
                    'å·²æ£€æµ‹': len(self.results),
                    'å¹³å‡æ—¶é—´': f"{np.mean(self.detection_stats['inference_times']):.3f}s"
                })
                
            except Exception as e:
                print(f"âŒ å¤„ç†å›¾ç‰‡ {image_path} æ—¶å‡ºé”™: {e}")
                continue
        
        print("âœ… æ‰¹é‡æµ‹è¯•å®Œæˆ!")
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_test_report(confidence_threshold, iou_threshold)
        
        # åœ¨ç”ŸæˆæŠ¥å‘Šåæ·»åŠ YOLOæ ¼å¼å¯¼å‡º
        self._generate_yolo_format_export()

        return self.results
    
    def _process_single_image_results(self, results, image_path, inference_time):
        """å¤„ç†å•å¼ å›¾ç‰‡çš„æ£€æµ‹ç»“æœ"""
        image_results = {
            "image_path": image_path,
            "image_name": Path(image_path).name,
            "inference_time": inference_time,
            "detections": [],
            "detection_count": 0,
            "categories_found": set(),
            "image_size": None
        }
        
        for result in results:
            # è·å–å›¾ç‰‡å°ºå¯¸
            if result.orig_shape is not None:
                image_results["image_size"] = result.orig_shape  # (height, width)

            if result.boxes is not None:
                for box in result.boxes:
                    class_id = int(box.cls[0])
                    class_name = self.model.names[class_id]
                    confidence = float(box.conf[0])
                    bbox = box.xyxy[0].tolist()
                    
                    # è·å–åƒåœ¾åˆ†ç±»
                    garbage_category = self.garbage_categories.get(class_name, "æœªçŸ¥åˆ†ç±»")
                    
                    detection_info = {
                        "class_name": class_name,
                        "category": garbage_category,
                        "confidence": confidence,
                        "bbox": bbox,
                        "bbox_normalized": None,
                        "area": (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
                    }

                    # è®¡ç®—å½’ä¸€åŒ–è¾¹ç•Œæ¡† (YOLOæ ¼å¼)
                    if image_results["image_size"] is not None:
                        img_h, img_w = image_results["image_size"]
                        x_center = (bbox[0] + bbox[2]) / 2 / img_w
                        y_center = (bbox[1] + bbox[3]) / 2 / img_h
                        width = (bbox[2] - bbox[0]) / img_w
                        height = (bbox[3] - bbox[1]) / img_h
                        detection_info["bbox_normalized"] = [x_center, y_center, width, height]
                    
                    image_results["detections"].append(detection_info)
                    image_results["categories_found"].add(garbage_category)
                    
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    self.detection_stats["total_detections"] += 1
                    self.detection_stats["category_counts"][garbage_category] = \
                        self.detection_stats["category_counts"].get(garbage_category, 0) + 1
                    self.detection_stats["confidence_stats"].append(confidence)
        
        image_results["detection_count"] = len(image_results["detections"])
        image_results["categories_found"] = list(image_results["categories_found"])
        
        return image_results
    
    def _generate_test_report(self, confidence_threshold, iou_threshold):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        # 1. ä¿å­˜åŸå§‹ç»“æœ
        self._save_raw_results()
        
        # 2. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_statistical_report(confidence_threshold, iou_threshold)
        
        # 3. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_visualizations()
        
        # 4. ç”ŸæˆYOLOæ ¼å¼çš„è¯„ä¼°æŒ‡æ ‡
        self._generate_yolo_metrics()
    
    def _save_raw_results(self):
        """ä¿å­˜åŸå§‹æ£€æµ‹ç»“æœ"""
        # ä¿å­˜ä¸ºJSON
        with open(self.output_dir / "raw_detection_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸ºCSV
        csv_data = []
        for result in self.results:
            for detection in result["detections"]:
                csv_data.append({
                    "image_path": result["image_path"],
                    "image_name": result["image_name"],
                    "class_name": detection["class_name"],
                    "category": detection["category"],
                    "confidence": detection["confidence"],
                    "inference_time": result["inference_time"]
                })
        
        df = pd.DataFrame(csv_data)
        df.to_csv(self.output_dir / "detection_results.csv", index=False, encoding="utf-8")
    
    def _generate_statistical_report(self, confidence_threshold, iou_threshold):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        stats = self.detection_stats
        
        # åŸºæœ¬ç»Ÿè®¡
        report = {
            "æµ‹è¯•é…ç½®": {
                "æ¨¡å‹è·¯å¾„": str(self.model.ckpt_path),
                "æµ‹è¯•å›¾ç‰‡æ•°é‡": stats["total_images"],
                "ç½®ä¿¡åº¦é˜ˆå€¼": confidence_threshold,
                "IoUé˜ˆå€¼": iou_threshold,
                "æ€»æ£€æµ‹æ•°é‡": stats["total_detections"]
            },
            "æ€§èƒ½ç»Ÿè®¡": {
                "å¹³å‡æ¨ç†æ—¶é—´": f"{np.mean(stats['inference_times']):.4f}ç§’",
                "æœ€å¿«æ¨ç†æ—¶é—´": f"{np.min(stats['inference_times']):.4f}ç§’",
                "æœ€æ…¢æ¨ç†æ—¶é—´": f"{np.max(stats['inference_times']):.4f}ç§’",
                "FPS": f"{1/np.mean(stats['inference_times']):.2f}",
                "æ€»æµ‹è¯•æ—¶é—´": f"{np.sum(stats['inference_times']):.2f}ç§’"
            },
            "æ£€æµ‹ç»Ÿè®¡": {
                "å¹³å‡æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°": f"{stats['total_detections'] / stats['total_images']:.2f}",
                "æ£€æµ‹ç‡": f"{(sum(1 for r in self.results if r['detection_count'] > 0) / stats['total_images']) * 100:.2f}%",
                "å¹³å‡ç½®ä¿¡åº¦": f"{np.mean(stats['confidence_stats']):.4f}",
                "ç½®ä¿¡åº¦æ ‡å‡†å·®": f"{np.std(stats['confidence_stats']):.4f}"
            },
            "åˆ†ç±»ç»Ÿè®¡": stats["category_counts"]
        }
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        with open(self.output_dir / "statistical_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(report)
    
    def _generate_markdown_report(self, report):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        md_content = f"""# åƒåœ¾æ£€æµ‹æ‰¹é‡æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•é…ç½®
- **æ¨¡å‹**: {report['æµ‹è¯•é…ç½®']['æ¨¡å‹è·¯å¾„']}
- **æµ‹è¯•å›¾ç‰‡æ•°é‡**: {report['æµ‹è¯•é…ç½®']['æµ‹è¯•å›¾ç‰‡æ•°é‡']}
- **ç½®ä¿¡åº¦é˜ˆå€¼**: {report['æµ‹è¯•é…ç½®']['ç½®ä¿¡åº¦é˜ˆå€¼']}
- **IoUé˜ˆå€¼**: {report['æµ‹è¯•é…ç½®']['IoUé˜ˆå€¼']}
- **æ€»æ£€æµ‹æ•°é‡**: {report['æµ‹è¯•é…ç½®']['æ€»æ£€æµ‹æ•°é‡']}

## æ€§èƒ½ç»Ÿè®¡
- **å¹³å‡æ¨ç†æ—¶é—´**: {report['æ€§èƒ½ç»Ÿè®¡']['å¹³å‡æ¨ç†æ—¶é—´']}
- **FPS**: {report['æ€§èƒ½ç»Ÿè®¡']['FPS']}
- **æ€»æµ‹è¯•æ—¶é—´**: {report['æ€§èƒ½ç»Ÿè®¡']['æ€»æµ‹è¯•æ—¶é—´']}

## æ£€æµ‹ç»Ÿè®¡
- **å¹³å‡æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°**: {report['æ£€æµ‹ç»Ÿè®¡']['å¹³å‡æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°']}
- **æ£€æµ‹ç‡**: {report['æ£€æµ‹ç»Ÿè®¡']['æ£€æµ‹ç‡']}
- **å¹³å‡ç½®ä¿¡åº¦**: {report['æ£€æµ‹ç»Ÿè®¡']['å¹³å‡ç½®ä¿¡åº¦']}

## åƒåœ¾åˆ†ç±»ç»Ÿè®¡
"""
        
        for category, count in report['åˆ†ç±»ç»Ÿè®¡'].items():
            percentage = (count / report['æµ‹è¯•é…ç½®']['æ€»æ£€æµ‹æ•°é‡']) * 100
            md_content += f"- **{category}**: {count} ä¸ª ({percentage:.1f}%)\\n"
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        with open(self.output_dir / "test_report.md", "w", encoding="utf-8") as f:
            f.write(md_content)
    
    def _generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. åƒåœ¾åˆ†ç±»åˆ†å¸ƒé¥¼å›¾
        self._plot_category_distribution()
        
        # 2. æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
        self._plot_confidence_distribution()
        
        # 3. æ¨ç†æ—¶é—´åˆ†å¸ƒå›¾
        self._plot_inference_time_distribution()
        
        # 4. æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°é‡åˆ†å¸ƒ
        self._plot_detections_per_image()
        
        # 5. å„ç±»åˆ«æ£€æµ‹æ•°é‡æŸ±çŠ¶å›¾
        self._plot_category_bar_chart()
    
    def _plot_category_distribution(self):
        """ç»˜åˆ¶åƒåœ¾åˆ†ç±»åˆ†å¸ƒé¥¼å›¾"""
        category_counts = self.detection_stats["category_counts"]
        
        if not category_counts:
            return
        
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = ['#4CAF50', '#F44336', '#FF9800', '#9E9E9E', '#2196F3']
        
        wedges, texts, autotexts = ax.pie(
            category_counts.values(),
            labels=category_counts.keys(),
            autopct='%1.1f%%',
            startangle=90,
            colors=colors[:len(category_counts)]
        )
        
        # ç¾åŒ–æ–‡æœ¬
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
        
        ax.set_title('åƒåœ¾åˆ†ç±»åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "category_distribution.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_confidence_distribution(self):
        """ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾"""
        if not self.detection_stats["confidence_stats"]:
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.hist(self.detection_stats["confidence_stats"], bins=20, 
               alpha=0.7, color='skyblue', edgecolor='black')
        
        ax.set_xlabel('ç½®ä¿¡åº¦')
        ax.set_ylabel('é¢‘ç‡')
        ax.set_title('æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_conf = np.mean(self.detection_stats["confidence_stats"])
        ax.axvline(mean_conf, color='red', linestyle='--', 
                  label=f'å¹³å‡ç½®ä¿¡åº¦: {mean_conf:.3f}')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "confidence_distribution.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_inference_time_distribution(self):
        """ç»˜åˆ¶æ¨ç†æ—¶é—´åˆ†å¸ƒå›¾"""
        if not self.detection_stats["inference_times"]:
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.hist(self.detection_stats["inference_times"], bins=20, 
               alpha=0.7, color='lightgreen', edgecolor='black')
        
        ax.set_xlabel('æ¨ç†æ—¶é—´ (ç§’)')
        ax.set_ylabel('é¢‘ç‡')
        ax.set_title('æ¨ç†æ—¶é—´åˆ†å¸ƒ')
        ax.grid(True, alpha=0.3)
        
        mean_time = np.mean(self.detection_stats["inference_times"])
        ax.axvline(mean_time, color='red', linestyle='--', 
                  label=f'å¹³å‡æ—¶é—´: {mean_time:.3f}s')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "inference_time_distribution.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_detections_per_image(self):
        """ç»˜åˆ¶æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°é‡åˆ†å¸ƒ"""
        detections_per_image = [r["detection_count"] for r in self.results]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.hist(detections_per_image, bins=20, alpha=0.7, 
               color='orange', edgecolor='black')
        
        ax.set_xlabel('æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°é‡')
        ax.set_ylabel('å›¾ç‰‡æ•°é‡')
        ax.set_title('æ¯å¼ å›¾ç‰‡æ£€æµ‹æ•°é‡åˆ†å¸ƒ')
        ax.grid(True, alpha=0.3)
        
        mean_detections = np.mean(detections_per_image)
        ax.axvline(mean_detections, color='red', linestyle='--', 
                  label=f'å¹³å‡æ£€æµ‹æ•°: {mean_detections:.2f}')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "detections_per_image.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_category_bar_chart(self):
        """ç»˜åˆ¶å„ç±»åˆ«æ£€æµ‹æ•°é‡æŸ±çŠ¶å›¾"""
        category_counts = self.detection_stats["category_counts"]
        
        if not category_counts:
            return
        
        fig, ax = plt.subplots(figsize=(12, 6))
        categories = list(category_counts.keys())
        counts = list(category_counts.values())
        
        bars = ax.bar(categories, counts, color=['#4CAF50', '#F44336', '#FF9800', '#9E9E9E'])
        
        ax.set_xlabel('åƒåœ¾ç±»åˆ«')
        ax.set_ylabel('æ£€æµ‹æ•°é‡')
        ax.set_title('å„ç±»åˆ«æ£€æµ‹æ•°é‡')
        ax.tick_params(axis='x', rotation=45)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{count}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "visualizations" / "category_bar_chart.png", 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_yolo_metrics(self):
        """ç”ŸæˆYOLOæ ¼å¼çš„è¯„ä¼°æŒ‡æ ‡"""
        # å¦‚æœæœ‰æ ‡æ³¨æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨YOLOçš„valæ–¹æ³•
        # è¿™é‡Œæˆ‘ä»¬ç”Ÿæˆä¸€äº›åŸºäºæ£€æµ‹ç»“æœçš„è¡ç”ŸæŒ‡æ ‡
        
        metrics = {
            "æ£€æµ‹è¦†ç›–ç‡": self._calculate_detection_coverage(),
            "ç±»åˆ«å¹³è¡¡åº¦": self._calculate_category_balance(),
            "æ£€æµ‹ç¨³å®šæ€§": self._calculate_detection_stability(),
            "æ€§èƒ½æŒ‡æ ‡": {
                "å¹³å‡FPS": 1 / np.mean(self.detection_stats["inference_times"]),
                "ååé‡": len(self.results) / np.sum(self.detection_stats["inference_times"]),
                "æ£€æµ‹å¯†åº¦": self.detection_stats["total_detections"] / self.detection_stats["total_images"]
            }
        }
        
        with open(self.output_dir / "metrics" / "yolo_style_metrics.json", "w") as f:
            json.dump(metrics, f, indent=2)
    
    def _calculate_detection_coverage(self):
        """è®¡ç®—æ£€æµ‹è¦†ç›–ç‡"""
        images_with_detections = sum(1 for r in self.results if r["detection_count"] > 0)
        return images_with_detections / self.detection_stats["total_images"]
    
    def _calculate_category_balance(self):
        """è®¡ç®—ç±»åˆ«å¹³è¡¡åº¦"""
        category_counts = list(self.detection_stats["category_counts"].values())
        if not category_counts:
            return 0
        return min(category_counts) / max(category_counts)
    
    def _calculate_detection_stability(self):
        """è®¡ç®—æ£€æµ‹ç¨³å®šæ€§ï¼ˆæ£€æµ‹æ•°é‡çš„å˜å¼‚ç³»æ•°ï¼‰"""
        detection_counts = [r["detection_count"] for r in self.results]
        if not detection_counts:
            return 0
        return np.std(detection_counts) / np.mean(detection_counts)
    
    def get_summary(self):
        """è·å–æµ‹è¯•æ‘˜è¦"""
        return {
            "total_images": self.detection_stats["total_images"],
            "total_detections": self.detection_stats["total_detections"],
            "avg_inference_time": np.mean(self.detection_stats["inference_times"]),
            "avg_confidence": np.mean(self.detection_stats["confidence_stats"]),
            "category_breakdown": self.detection_stats["category_counts"]
        }
    
    def _generate_yolo_format_export(self):
        """ç”ŸæˆYOLOæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶å’Œæ•°æ®é›†é…ç½®"""
        print("ğŸ“ ç”ŸæˆYOLOæ ¼å¼æ ‡æ³¨æ–‡ä»¶...")
        
        # 1. ç”ŸæˆYOLOæ ¼å¼çš„æ ‡ç­¾æ–‡ä»¶
        self._generate_yolo_labels()
        
        # 2. ç”Ÿæˆæ•°æ®é›†YAMLé…ç½®æ–‡ä»¶
        self._generate_dataset_yaml()
        
        # 3. åˆ›å»ºæ•°æ®é›†çš„ç›®å½•ç»“æ„
        self._create_dataset_structure()
    
    def _generate_yolo_labels(self):
        """ç”ŸæˆYOLOæ ¼å¼çš„æ ‡ç­¾æ–‡ä»¶"""
        labels_dir = self.output_dir / "yolo_labels"
        
        for result in self.results:
            if result["detections"] and result["image_size"] is not None:
                # ç”Ÿæˆå¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶å
                label_filename = Path(result["image_name"]).stem + ".txt"
                label_path = labels_dir / label_filename
                
                with open(label_path, "w", encoding="utf-8") as f:
                    for detection in result["detections"]:
                        if detection["bbox_normalized"] is not None:
                            x_center, y_center, width, height = detection["bbox_normalized"]
                            class_id = self.class_to_id.get(detection["class_name"], 0)
                            
                            # YOLOæ ¼å¼: class_id x_center y_center width height
                            line = f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n"
                            f.write(line)
    
    def _generate_dataset_yaml(self):
        """ç”Ÿæˆæ•°æ®é›†YAMLé…ç½®æ–‡ä»¶"""
        dataset_config = {
            'path': str(self.output_dir / "yolo_format"),
            'train': 'images/train',
            'val': 'images/val',
            'test': 'images/test',
            'names': {idx: name for name, idx in self.class_to_id.items()}
        }
        
        yaml_path = self.output_dir / "yolo_format" / "dataset.yaml"
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(dataset_config, f, default_flow_style=False, allow_unicode=True)
        
        # åŒæ—¶ä¿å­˜ä¸ºJSONæ ¼å¼ä¾¿äºå…¶ä»–å·¥å…·ä½¿ç”¨
        dataset_info = {
            "dataset_info": {
                "name": "garbage_detection_dataset",
                "description": "è‡ªåŠ¨ç”Ÿæˆçš„åƒåœ¾åˆ†ç±»æ£€æµ‹æ•°æ®é›†",
                "created": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_images": len(self.results),
                "total_detections": self.detection_stats["total_detections"],
                "classes": list(self.class_to_id.keys()),
                "class_mapping": self.class_to_id,
                "garbage_categories": self.garbage_categories
            },
            "paths": {
                "images_dir": "images",
                "labels_dir": "labels",
                "original_images": str(self.test_images_dir)
            }
        }
        
        json_path = self.output_dir / "yolo_format" / "dataset_info.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
    
    def _create_dataset_structure(self):
        """åˆ›å»ºæ ‡å‡†çš„æ•°æ®é›†ç›®å½•ç»“æ„"""
        yolo_dir = self.output_dir / "yolo_format"
        
        # åˆ›å»ºæ ‡å‡†ç›®å½•ç»“æ„
        (yolo_dir / "images" / "train").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "images" / "val").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "images" / "test").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "labels" / "train").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "labels" / "val").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "labels" / "test").mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶æ ‡ç­¾æ–‡ä»¶åˆ°labels/trainç›®å½•ï¼ˆé»˜è®¤å…¨éƒ¨ä½œä¸ºè®­ç»ƒé›†ï¼‰
        labels_dir = self.output_dir / "yolo_labels"
        target_labels_dir = yolo_dir / "labels" / "train"
        
        for label_file in labels_dir.glob("*.txt"):
            target_path = target_labels_dir / label_file.name
            if target_path.exists():
                target_path.unlink()
            label_file.rename(target_path)
        
        # åˆ›å»ºå›¾ç‰‡æ–‡ä»¶çš„ç¬¦å·é“¾æ¥ï¼ˆå¦‚æœå¯èƒ½ï¼‰æˆ–è®°å½•å›¾ç‰‡è·¯å¾„
        self._create_image_links_or_list(yolo_dir)
    
    def _create_image_links_or_list(self, yolo_dir):
        """åˆ›å»ºå›¾ç‰‡é“¾æ¥æˆ–ç”Ÿæˆå›¾ç‰‡è·¯å¾„åˆ—è¡¨"""
        # æ–¹æ³•1: å°è¯•åˆ›å»ºç¬¦å·é“¾æ¥
        try:
            target_images_dir = yolo_dir / "images" / "train"
            for result in self.results:
                image_path = Path(result["image_path"])
                if image_path.exists():
                    link_path = target_images_dir / result["image_name"]
                    if not link_path.exists():
                        # åœ¨Windowsä¸Šå¯èƒ½éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œæ‰€ä»¥å°è¯•å¤åˆ¶
                        import shutil
                        shutil.copy2(image_path, link_path)
            print("âœ… å›¾ç‰‡æ–‡ä»¶å·²å¤åˆ¶åˆ°YOLOæ ¼å¼ç›®å½•")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åˆ›å»ºå›¾ç‰‡é“¾æ¥/å¤åˆ¶æ–‡ä»¶: {e}")
            # æ–¹æ³•2: ç”Ÿæˆå›¾ç‰‡è·¯å¾„åˆ—è¡¨æ–‡ä»¶
            self._generate_image_list_file(yolo_dir)
    
    def _generate_image_list_file(self, yolo_dir):
        """ç”ŸæˆåŒ…å«å›¾ç‰‡è·¯å¾„çš„åˆ—è¡¨æ–‡ä»¶"""
        # ç”Ÿæˆè®­ç»ƒé›†å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        train_list_path = yolo_dir / "train.txt"
        with open(train_list_path, 'w', encoding='utf-8') as f:
            for result in self.results:
                f.write(str(Path(result["image_path"]).absolute()) + '\n')
        
        print("âœ… å·²ç”Ÿæˆå›¾ç‰‡è·¯å¾„åˆ—è¡¨æ–‡ä»¶ (train.txt)")

    # =======================å¯¹æ ‡ç­¾æµ‹è¯•=====================
    def evaluate_with_labels(self, labels_dir, data_yaml=None, batch_size=16, imgsz=640, 
                           conf_threshold=0.001, iou_threshold=0.6, save_json=False):
        """
        ä½¿ç”¨çœŸå®æ ‡ç­¾è¿›è¡Œè¯„ä¼°ï¼Œè®¡ç®—å„ç§æŒ‡æ ‡
        
        labels_dir: çœŸå®æ ‡ç­¾ç›®å½•
        data_yaml: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
        batch_size: æ‰¹å¤„ç†å¤§å°
        imgsz: å›¾åƒå°ºå¯¸
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        iou_threshold: IoUé˜ˆå€¼
        save_json: æ˜¯å¦ä¿å­˜JSONæ ¼å¼ç»“æœ
        """
        print("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°ï¼ˆä½¿ç”¨çœŸå®æ ‡ç­¾ï¼‰...")
        
        # éªŒè¯æ ‡ç­¾ç›®å½•å­˜åœ¨
        labels_path = Path(labels_dir)
        if not labels_path.exists():
            print(f"âŒ æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {labels_dir}")
            return None
        
        # å¦‚æœæœªæä¾›data_yamlï¼Œå°è¯•è‡ªåŠ¨åˆ›å»º
        if data_yaml is None:
            data_yaml = self._create_evaluation_data_yaml(labels_dir)
        
        # æ‰§è¡Œè¯„ä¼°
        try:
            results = self.model.val(
                data=data_yaml,
                batch=batch_size,
                imgsz=imgsz,
                conf=conf_threshold,
                iou=iou_threshold,
                save_json=save_json,
                project=str(self.output_dir / "evaluation"),
                name="val_results",
                exist_ok=True
            )
            
            # å¤„ç†è¯„ä¼°ç»“æœ
            self._process_evaluation_results(results)
            
            print("âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ!")
            return results
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return None
    
    def _create_evaluation_data_yaml(self, labels_dir):
        """ä¸ºè¯„ä¼°åˆ›å»ºä¸´æ—¶çš„æ•°æ®é›†é…ç½®æ–‡ä»¶"""
        eval_data = {
            'path': str(Path(labels_dir).parent),  # å‡è®¾å›¾ç‰‡å’Œæ ‡ç­¾åœ¨åŒä¸€ä¸ªçˆ¶ç›®å½•ä¸‹
            'train': None,
            'val': str(self.test_images_dir),
            'test': None,
            'nc': len(self.class_to_id),
            'names': {v: k for k, v in self.class_to_id.items()}
        }
        
        # ä¿å­˜ä¸´æ—¶YAMLæ–‡ä»¶
        temp_yaml = self.output_dir / "evaluation" / "temp_data.yaml"
        with open(temp_yaml, 'w', encoding='utf-8') as f:
            yaml.dump(eval_data, f, default_flow_style=False, allow_unicode=True)
        
        return str(temp_yaml)
    
    def _process_evaluation_results(self, results):
        """å¤„ç†è¯„ä¼°ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨"""
        if results is None:
            return
        
        # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
        metrics = {
            "precision": results.box.map50,  # ä½¿ç”¨mAP50ä½œä¸ºprecisionçš„è¿‘ä¼¼
            "recall": getattr(results, 'recall', None),
            "map50": results.box.map50,
            "map50_95": results.box.map,
            "f1_score": self._calculate_f1_score(results),
            "losses": {
                "box_loss": getattr(results, 'box_loss', None),
                "cls_loss": getattr(results, 'cls_loss', None),
                "dfl_loss": getattr(results, 'dfl_loss', None),
            },
            "speed": {
                "preprocess": getattr(results, 'speed', {}).get('preprocess', None),
                "inference": getattr(results, 'speed', {}).get('inference', None),
                "postprocess": getattr(results, 'speed', {}).get('postprocess', None),
            },
            "per_class_metrics": self._extract_per_class_metrics(results)
        }
        
        # ä¿å­˜æŒ‡æ ‡
        with open(self.output_dir / "evaluation" / "metrics.json", "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        self._generate_evaluation_report(metrics)
        
        # ç”Ÿæˆè¯„ä¼°å›¾è¡¨
        self._generate_evaluation_plots(results, metrics)
    
    def _calculate_f1_score(self, results):
        """è®¡ç®—F1åˆ†æ•°"""
        precision = results.box.map50  # è¿‘ä¼¼å€¼
        recall = getattr(results, 'recall', None)
        
        if precision and recall and (precision + recall) > 0:
            return 2 * (precision * recall) / (precision + recall)
        return None
    
    def _extract_per_class_metrics(self, results):
        """æå–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡"""
        per_class_metrics = {}
        
        # å°è¯•ä»ç»“æœä¸­è·å–æ¯ä¸ªç±»åˆ«çš„AP
        if hasattr(results, 'results_dict') and 'results_per_class' in results.results_dict:
            for class_name, class_metrics in results.results_dict['results_per_class'].items():
                per_class_metrics[class_name] = {
                    "precision": class_metrics.get('precision', None),
                    "recall": class_metrics.get('recall', None),
                    "ap50": class_metrics.get('AP50', None),
                    "ap50_95": class_metrics.get('AP', None)
                }
        
        return per_class_metrics
    
    def _generate_evaluation_report(self, metrics):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = {
            "è¯„ä¼°æ‘˜è¦": {
                "æ¨¡å‹è·¯å¾„": str(self.model.ckpt_path),
                "æµ‹è¯•å›¾ç‰‡æ•°é‡": self.detection_stats["total_images"],
                "è¯„ä¼°æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "æ€§èƒ½æŒ‡æ ‡": {
                "mAP@0.5": f"{metrics['map50']:.4f}",
                "mAP@0.5:0.95": f"{metrics['map50_95']:.4f}",
                "ç²¾ç¡®ç‡": f"{metrics['precision']:.4f}" if metrics['precision'] else "N/A",
                "å¬å›ç‡": f"{metrics['recall']:.4f}" if metrics['recall'] else "N/A",
                "F1åˆ†æ•°": f"{metrics['f1_score']:.4f}" if metrics['f1_score'] else "N/A"
            },
            "æŸå¤±å€¼": metrics['losses'],
            "æ¨ç†é€Ÿåº¦": metrics['speed']
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(self.output_dir / "evaluation" / "evaluation_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_evaluation_markdown_report(report, metrics)
    
    def _generate_evaluation_markdown_report(self, report, metrics):
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        md_content = f"""# æ¨¡å‹è¯„ä¼°æŠ¥å‘Š

## è¯„ä¼°é…ç½®
- **æ¨¡å‹**: {report['è¯„ä¼°æ‘˜è¦']['æ¨¡å‹è·¯å¾„']}
- **æµ‹è¯•å›¾ç‰‡æ•°é‡**: {report['è¯„ä¼°æ‘˜è¦']['æµ‹è¯•å›¾ç‰‡æ•°é‡']}
- **è¯„ä¼°æ—¶é—´**: {report['è¯„ä¼°æ‘˜è¦']['è¯„ä¼°æ—¶é—´']}

## æ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | å€¼ |
|------|-----|
| mAP@0.5 | {report['æ€§èƒ½æŒ‡æ ‡']['mAP@0.5']} |
| mAP@0.5:0.95 | {report['æ€§èƒ½æŒ‡æ ‡']['mAP@0.5:0.95']} |
| ç²¾ç¡®ç‡ | {report['æ€§èƒ½æŒ‡æ ‡']['ç²¾ç¡®ç‡']} |
| å¬å›ç‡ | {report['æ€§èƒ½æŒ‡æ ‡']['å¬å›ç‡']} |
| F1åˆ†æ•° | {report['æ€§èƒ½æŒ‡æ ‡']['F1åˆ†æ•°']} |

## æŸå¤±å€¼
"""
        
        for loss_name, loss_value in report['æŸå¤±å€¼'].items():
            if loss_value is not None:
                md_content += f"- **{loss_name}**: {loss_value:.4f}\\n"
            else:
                md_content += f"- **{loss_name}**: N/A\\n"
        
        md_content += "\n## æ¨ç†é€Ÿåº¦\\n"
        for speed_name, speed_value in report['æ¨ç†é€Ÿåº¦'].items():
            if speed_value is not None:
                md_content += f"- **{speed_name}**: {speed_value:.4f} ms/image\\n"
            else:
                md_content += f"- **{speed_name}**: N/A\\n"
        
        # æ·»åŠ æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        if metrics['per_class_metrics']:
            md_content += "\n## æ¯ä¸ªç±»åˆ«æ€§èƒ½\\n"
            md_content += "| ç±»åˆ« | AP@0.5 | AP@0.5:0.95 |\\n"
            md_content += "|------|--------|-------------|\\n"
            
            for class_name, class_metrics in metrics['per_class_metrics'].items():
                ap50 = class_metrics.get('ap50', 'N/A')
                ap50_95 = class_metrics.get('ap50_95', 'N/A')
                
                if ap50 != 'N/A':
                    ap50 = f"{ap50:.4f}"
                if ap50_95 != 'N/A':
                    ap50_95 = f"{ap50_95:.4f}"
                
                md_content += f"| {class_name} | {ap50} | {ap50_95} |\\n"
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        with open(self.output_dir / "evaluation" / "evaluation_report.md", "w", encoding="utf-8") as f:
            f.write(md_content)
    
    def _generate_evaluation_plots(self, results, metrics):
        """ç”Ÿæˆè¯„ä¼°å›¾è¡¨"""
        print("ğŸ“ˆ ç”Ÿæˆè¯„ä¼°å›¾è¡¨...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. ç»˜åˆ¶PRæ›²çº¿
        self._plot_pr_curve(results)
        
        # 2. ç»˜åˆ¶F1æ›²çº¿
        self._plot_f1_curve(results)
        
        # 3. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        self._plot_confusion_matrix(results)
        
        # 4. ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„AP
        self._plot_per_class_ap(results)
        
        # 5. ç»˜åˆ¶æŸå¤±æ›²çº¿ï¼ˆå¦‚æœæœ‰è®­ç»ƒå†å²ï¼‰
        self._plot_loss_curves_if_available()
        
        # 6. ç»˜åˆ¶æŒ‡æ ‡é›·è¾¾å›¾
        self._plot_metrics_radar(metrics)
    
    def _plot_pr_curve(self, results):
        """ç»˜åˆ¶PRæ›²çº¿"""
        try:
            # å°è¯•ä»ç»“æœä¸­è·å–PRæ›²çº¿æ•°æ®
            if hasattr(results, 'pr_curve'):
                pr_curve_data = results.pr_curve
                
                fig, ax = plt.subplots(figsize=(10, 8))
                ax.plot(pr_curve_data[0], pr_curve_data[1], linewidth=2, color='blue')
                ax.set_xlabel('Recall')
                ax.set_ylabel('Precision')
                ax.set_title('Precision-Recall Curve')
                ax.grid(True, alpha=0.3)
                ax.set_xlim(0, 1)
                ax.set_ylim(0, 1)
                
                # æ·»åŠ APå€¼
                ap = getattr(results, 'box', {}).get('map50', 0)
                ax.text(0.6, 0.1, f'mAP@0.5: {ap:.3f}', 
                       bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
                
                plt.tight_layout()
                plt.savefig(self.output_dir / "evaluation" / "plots" / "pr_curve.png", 
                           dpi=300, bbox_inches='tight')
                plt.close()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ç»˜åˆ¶PRæ›²çº¿: {e}")
    
    def _plot_f1_curve(self, results):
        """ç»˜åˆ¶F1æ›²çº¿"""
        try:
            # å°è¯•ä»ç»“æœä¸­è·å–F1æ›²çº¿æ•°æ®
            if hasattr(results, 'f1_curve'):
                f1_curve_data = results.f1_curve
                
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.plot(f1_curve_data[0], f1_curve_data[1], linewidth=2, color='green')
                ax.set_xlabel('Confidence Threshold')
                ax.set_ylabel('F1 Score')
                ax.set_title('F1-Confidence Curve')
                ax.grid(True, alpha=0.3)
                
                # æ‰¾åˆ°æœ€ä½³F1åˆ†æ•°å’Œå¯¹åº”çš„ç½®ä¿¡åº¦é˜ˆå€¼
                if len(f1_curve_data[1]) > 0:
                    best_f1_idx = np.argmax(f1_curve_data[1])
                    best_f1 = f1_curve_data[1][best_f1_idx]
                    best_conf = f1_curve_data[0][best_f1_idx]
                    
                    ax.axvline(best_conf, color='red', linestyle='--', 
                              label=f'æœ€ä½³é˜ˆå€¼: {best_conf:.3f} (F1={best_f1:.3f})')
                    ax.legend()
                
                plt.tight_layout()
                plt.savefig(self.output_dir / "evaluation" / "plots" / "f1_curve.png", 
                           dpi=300, bbox_inches='tight')
                plt.close()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ç»˜åˆ¶F1æ›²çº¿: {e}")
    
    def _plot_confusion_matrix(self, results):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        try:
            if hasattr(results, 'confusion_matrix'):
                cm = results.confusion_matrix
                
                fig, ax = plt.subplots(figsize=(12, 10))
                
                # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
                cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
                
                # è·å–ç±»åˆ«åç§°
                class_names = list(self.class_to_id.keys())
                if len(class_names) < cm.shape[0]:
                    class_names = [f"Class {i}" for i in range(cm.shape[0])]
                
                # ç»˜åˆ¶çƒ­åŠ›å›¾
                im = ax.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)
                ax.figure.colorbar(im, ax=ax)
                
                # è®¾ç½®åˆ»åº¦
                ax.set(xticks=np.arange(cm_normalized.shape[1]),
                      yticks=np.arange(cm_normalized.shape[0]),
                      xticklabels=class_names,
                      yticklabels=class_names,
                      title="æ··æ·†çŸ©é˜µ (å½’ä¸€åŒ–)",
                      ylabel="çœŸå®æ ‡ç­¾",
                      xlabel="é¢„æµ‹æ ‡ç­¾")
                
                # æ—‹è½¬xè½´æ ‡ç­¾
                plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
                
                # åœ¨æ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼
                thresh = cm_normalized.max() / 2.
                for i in range(cm_normalized.shape[0]):
                    for j in range(cm_normalized.shape[1]):
                        ax.text(j, i, f"{cm_normalized[i, j]:.2f}",
                               ha="center", va="center",
                               color="white" if cm_normalized[i, j] > thresh else "black")
                
                plt.tight_layout()
                plt.savefig(self.output_dir / "evaluation" / "plots" / "confusion_matrix.png", 
                           dpi=300, bbox_inches='tight')
                plt.close()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ç»˜åˆ¶æ··æ·†çŸ©é˜µ: {e}")
    
    def _plot_per_class_ap(self, results):
        """ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„AP"""
        try:
            if metrics := getattr(results, 'results_dict', {}).get('results_per_class', {}):
                class_names = []
                ap50_values = []
                ap50_95_values = []
                
                for class_name, class_metrics in metrics.items():
                    class_names.append(class_name)
                    ap50_values.append(class_metrics.get('AP50', 0))
                    ap50_95_values.append(class_metrics.get('AP', 0))
                
                x = np.arange(len(class_names))
                width = 0.35
                
                fig, ax = plt.subplots(figsize=(12, 6))
                rects1 = ax.bar(x - width/2, ap50_values, width, label='AP@0.5', alpha=0.8)
                rects2 = ax.bar(x + width/2, ap50_95_values, width, label='AP@0.5:0.95', alpha=0.8)
                
                ax.set_xlabel('ç±»åˆ«')
                ax.set_ylabel('APå€¼')
                ax.set_title('æ¯ä¸ªç±»åˆ«çš„å¹³å‡ç²¾åº¦(AP)')
                ax.set_xticks(x)
                ax.set_xticklabels(class_names, rotation=45)
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                def autolabel(rects):
                    for rect in rects:
                        height = rect.get_height()
                        ax.annotate(f'{height:.3f}',
                                   xy=(rect.get_x() + rect.get_width() / 2, height),
                                   xytext=(0, 3),
                                   textcoords="offset points",
                                   ha='center', va='bottom', fontsize=8)
                
                autolabel(rects1)
                autolabel(rects2)
                
                plt.tight_layout()
                plt.savefig(self.output_dir / "evaluation" / "plots" / "per_class_ap.png", 
                           dpi=300, bbox_inches='tight')
                plt.close()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ç»˜åˆ¶æ¯ä¸ªç±»åˆ«APå›¾: {e}")
    
    def _plot_loss_curves_if_available(self):
        """å¦‚æœå¯ç”¨ï¼Œç»˜åˆ¶æŸå¤±æ›²çº¿"""
        try:
            # å°è¯•ä»æ¨¡å‹è·¯å¾„è·å–è®­ç»ƒå†å²
            model_dir = Path(self.model.ckpt_path).parent if self.model.ckpt_path else None
            if model_dir and (model_dir / "results.csv").exists():
                results_csv = pd.read_csv(model_dir / "results.csv")
                
                fig, axes = plt.subplots(2, 2, figsize=(15, 10))
                axes = axes.flatten()
                
                # ç»˜åˆ¶è®­ç»ƒæŸå¤±
                if 'train/box_loss' in results_csv.columns:
                    axes[0].plot(results_csv['train/box_loss'], label='Box Loss')
                    axes[0].plot(results_csv['train/cls_loss'], label='Cls Loss')
                    axes[0].plot(results_csv['train/dfl_loss'], label='DFL Loss')
                    axes[0].set_title('è®­ç»ƒæŸå¤±')
                    axes[0].legend()
                    axes[0].grid(True, alpha=0.3)
                
                # ç»˜åˆ¶éªŒè¯æŸå¤±
                if 'val/box_loss' in results_csv.columns:
                    axes[1].plot(results_csv['val/box_loss'], label='Box Loss')
                    axes[1].plot(results_csv['val/cls_loss'], label='Cls Loss')
                    axes[1].plot(results_csv['val/dfl_loss'], label='DFL Loss')
                    axes[1].set_title('éªŒè¯æŸå¤±')
                    axes[1].legend()
                    axes[1].grid(True, alpha=0.3)
                
                # ç»˜åˆ¶mAPæ›²çº¿
                if 'metrics/mAP50(B)' in results_csv.columns:
                    axes[2].plot(results_csv['metrics/mAP50(B)'], label='mAP@0.5')
                    axes[2].plot(results_csv['metrics/mAP50-95(B)'], label='mAP@0.5:0.95')
                    axes[2].set_title('mAPæŒ‡æ ‡')
                    axes[2].legend()
                    axes[2].grid(True, alpha=0.3)
                
                # ç»˜åˆ¶ç²¾ç¡®ç‡å’Œå¬å›ç‡
                if 'metrics/precision(B)' in results_csv.columns:
                    axes[3].plot(results_csv['metrics/precision(B)'], label='Precision')
                    axes[3].plot(results_csv['metrics/recall(B)'], label='Recall')
                    axes[3].set_title('ç²¾ç¡®ç‡å’Œå¬å›ç‡')
                    axes[3].legend()
                    axes[3].grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.savefig(self.output_dir / "evaluation" / "plots" / "loss_curves.png", 
                           dpi=300, bbox_inches='tight')
                plt.close()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ç»˜åˆ¶æŸå¤±æ›²çº¿: {e}")
    
    def _plot_metrics_radar(self, metrics):
        """ç»˜åˆ¶æŒ‡æ ‡é›·è¾¾å›¾"""
        try:
            # é€‰æ‹©è¦å±•ç¤ºçš„æŒ‡æ ‡
            radar_metrics = {
                'mAP@0.5': metrics.get('map50', 0),
                'mAP@0.5:0.95': metrics.get('map50_95', 0),
                'Precision': metrics.get('precision', 0),
                'Recall': metrics.get('recall', 0) or 0,
                'F1 Score': metrics.get('f1_score', 0) or 0
            }
            
            # è¿‡æ»¤æ‰å€¼ä¸ºNoneçš„æŒ‡æ ‡
            radar_metrics = {k: v for k, v in radar_metrics.items() if v is not None}
            
            if len(radar_metrics) >= 3:
                categories = list(radar_metrics.keys())
                values = list(radar_metrics.values())
                
                # å®Œæˆé›·è¾¾å›¾
                values += values[:1]
                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
                angles += angles[:1]
                
                fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
                ax.plot(angles, values, 'o-', linewidth=2, label='æ¨¡å‹æ€§èƒ½')
                ax.fill(angles, values, alpha=0.25)
                ax.set_xticks(angles[:-1])
                ax.set_xticklabels(categories)
                ax.set_ylim(0, 1)
                ax.set_title('æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾', size=14, y=1.05)
                ax.grid(True)
                ax.legend(loc='upper right')
                
                plt.tight_layout()
                plt.savefig(self.output_dir / "evaluation" / "plots" / "metrics_radar.png", 
                           dpi=300, bbox_inches='tight')
                plt.close()
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ç»˜åˆ¶æŒ‡æ ‡é›·è¾¾å›¾: {e}")