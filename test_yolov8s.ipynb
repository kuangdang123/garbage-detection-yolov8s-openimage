{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'test_config' from 'config' (d:\\Desktop\\工程实训\\config.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGarbageDetector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GarbageDetector \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test_config\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'test_config' from 'config' (d:\\Desktop\\工程实训\\config.py)"
     ]
    }
   ],
   "source": [
    "from GarbageDetector import GarbageDetector \n",
    "from config import test_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = GarbageDetector(model_path=test_config['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1000 d:\\Desktop\\\\export_oi\\images\\test\\000e4e7ed48c932d.jpg: 448x640 1 Orange, 427.9ms\n",
      "image 2/1000 d:\\Desktop\\\\export_oi\\images\\test\\001dd90fc5ae92c9.jpg: 448x640 4 Apples, 389.1ms\n",
      "image 3/1000 d:\\Desktop\\\\export_oi\\images\\test\\00423c885ef8e8b0.jpg: 640x480 4 Bananas, 327.9ms\n",
      "image 4/1000 d:\\Desktop\\\\export_oi\\images\\test\\004a9388fb6fd9fc.jpg: 640x512 1 Mobile phone, 412.8ms\n",
      "image 5/1000 d:\\Desktop\\\\export_oi\\images\\test\\004a9745418ba5a6.jpg: 640x448 1 Bottle, 424.0ms\n",
      "image 6/1000 d:\\Desktop\\\\export_oi\\images\\test\\004f40769dcc358f.jpg: 480x640 1 Bottle, 1 Book, 367.9ms\n",
      "image 7/1000 d:\\Desktop\\\\export_oi\\images\\test\\0079ae4be5f70d5b.jpg: 448x640 10 Oranges, 310.6ms\n",
      "image 8/1000 d:\\Desktop\\\\export_oi\\images\\test\\008ed2884d0fab85.jpg: 416x640 1 Coffee cup, 315.6ms\n",
      "image 9/1000 d:\\Desktop\\\\export_oi\\images\\test\\00a460645961977c.jpg: 480x640 2 Apples, 294.0ms\n",
      "image 10/1000 d:\\Desktop\\\\export_oi\\images\\test\\00b729b5187a1898.jpg: 640x480 1 Bottle, 283.2ms\n",
      "image 11/1000 d:\\Desktop\\\\export_oi\\images\\test\\00d03e88b1d2f7a3.jpg: 480x640 1 Coffee cup, 310.5ms\n",
      "image 12/1000 d:\\Desktop\\\\export_oi\\images\\test\\00d8bf5411fdfe51.jpg: 448x640 1 Mobile phone, 293.0ms\n",
      "image 13/1000 d:\\Desktop\\\\export_oi\\images\\test\\00f1fda298b88865.jpg: 448x640 1 Book, 285.3ms\n",
      "image 14/1000 d:\\Desktop\\\\export_oi\\images\\test\\010006993a980b92.jpg: 448x640 6 Bottles, 282.5ms\n",
      "image 15/1000 d:\\Desktop\\\\export_oi\\images\\test\\012161ba1f688d43.jpg: 480x640 1 Mobile phone, 1 Plastic bag, 300.6ms\n",
      "image 16/1000 d:\\Desktop\\\\export_oi\\images\\test\\0133239017fa03d7.jpg: 640x448 1 Mobile phone, 293.1ms\n",
      "image 17/1000 d:\\Desktop\\\\export_oi\\images\\test\\01495549e1f4fd2e.jpg: 640x480 1 Bottle, 300.7ms\n",
      "image 18/1000 d:\\Desktop\\\\export_oi\\images\\test\\014d078c2ea75e4c.jpg: 480x640 1 Coffee cup, 314.8ms\n",
      "image 19/1000 d:\\Desktop\\\\export_oi\\images\\test\\015567caba222a5d.jpg: 448x640 5 Apples, 292.4ms\n",
      "image 20/1000 d:\\Desktop\\\\export_oi\\images\\test\\01880029d632730e.jpg: 480x640 4 Books, 330.8ms\n",
      "image 21/1000 d:\\Desktop\\\\export_oi\\images\\test\\0192deb7b7fade4c.jpg: 480x640 1 Mobile phone, 296.0ms\n",
      "image 22/1000 d:\\Desktop\\\\export_oi\\images\\test\\01ac026097ecc699.jpg: 448x640 (no detections), 293.2ms\n",
      "image 23/1000 d:\\Desktop\\\\export_oi\\images\\test\\01bfc6a5c53e5086.jpg: 480x640 1 Mobile phone, 301.9ms\n",
      "image 24/1000 d:\\Desktop\\\\export_oi\\images\\test\\01c715ada9a9b193.jpg: 544x640 1 Coffee cup, 368.5ms\n",
      "image 25/1000 d:\\Desktop\\\\export_oi\\images\\test\\01f772f75d05477c.jpg: 448x640 11 Books, 289.9ms\n",
      "image 26/1000 d:\\Desktop\\\\export_oi\\images\\test\\01fdc956f59149b3.jpg: 448x640 15 Books, 370.6ms\n",
      "image 27/1000 d:\\Desktop\\\\export_oi\\images\\test\\023b447cac3b3557.jpg: 480x640 2 Books, 333.3ms\n",
      "image 28/1000 d:\\Desktop\\\\export_oi\\images\\test\\024fc048dd1cb61a.jpg: 640x640 1 Bottle, 434.9ms\n",
      "image 29/1000 d:\\Desktop\\\\export_oi\\images\\test\\0268212bd9a7224f.jpg: 480x640 1 Coffee cup, 460.5ms\n",
      "image 30/1000 d:\\Desktop\\\\export_oi\\images\\test\\028aa72dfbd6c15b.jpg: 512x640 2 Oranges, 422.6ms\n",
      "image 31/1000 d:\\Desktop\\\\export_oi\\images\\test\\02d301de1b793797.jpg: 448x640 1 Coffee cup, 274.5ms\n",
      "image 32/1000 d:\\Desktop\\\\export_oi\\images\\test\\032a985e731de7a1.jpg: 448x640 12 Bottles, 303.9ms\n",
      "image 33/1000 d:\\Desktop\\\\export_oi\\images\\test\\0339d9cff7ea0af4.jpg: 448x640 3 Bottles, 345.3ms\n",
      "image 34/1000 d:\\Desktop\\\\export_oi\\images\\test\\037015b5b6520ed1.jpg: 640x480 1 Bottle, 331.8ms\n",
      "image 35/1000 d:\\Desktop\\\\export_oi\\images\\test\\037753b46a434578.jpg: 576x640 1 Book, 380.9ms\n",
      "image 36/1000 d:\\Desktop\\\\export_oi\\images\\test\\03ccabc3fbd3bb15.jpg: 480x640 1 Apple, 3 Oranges, 296.6ms\n",
      "image 37/1000 d:\\Desktop\\\\export_oi\\images\\test\\03dd6c83eea901a3.jpg: 480x640 1 Book, 1 Mobile phone, 358.2ms\n",
      "image 38/1000 d:\\Desktop\\\\export_oi\\images\\test\\03e20f16eafbf89a.jpg: 480x640 1 Coffee cup, 328.1ms\n",
      "image 39/1000 d:\\Desktop\\\\export_oi\\images\\test\\03e3825b1b538688.jpg: 480x640 1 Coffee cup, 325.4ms\n",
      "image 40/1000 d:\\Desktop\\\\export_oi\\images\\test\\0403eebf8c0aac60.jpg: 640x608 2 Mobile phones, 403.2ms\n",
      "image 41/1000 d:\\Desktop\\\\export_oi\\images\\test\\044f1f1bc305606e.jpg: 448x640 1 Coffee cup, 354.0ms\n",
      "image 42/1000 d:\\Desktop\\\\export_oi\\images\\test\\0499037adbb9cdff.jpg: 640x640 3 Bottles, 509.1ms\n",
      "image 43/1000 d:\\Desktop\\\\export_oi\\images\\test\\049d99faa622b032.jpg: 640x416 2 Bottles, 379.7ms\n",
      "image 44/1000 d:\\Desktop\\\\export_oi\\images\\test\\04a504fe281c930e.jpg: 640x640 1 Mobile phone, 541.1ms\n",
      "image 45/1000 d:\\Desktop\\\\export_oi\\images\\test\\04c272c84b71c68e.jpg: 448x640 19 Bottles, 372.5ms\n",
      "image 46/1000 d:\\Desktop\\\\export_oi\\images\\test\\04cdaa5cff6efc2c.jpg: 448x640 1 Mobile phone, 326.4ms\n",
      "image 47/1000 d:\\Desktop\\\\export_oi\\images\\test\\04e9f42b51f842aa.jpg: 640x480 1 Mobile phone, 352.1ms\n",
      "image 48/1000 d:\\Desktop\\\\export_oi\\images\\test\\04fda7c9d8971ec0.jpg: 640x640 1 Apple, 2 Oranges, 490.3ms\n",
      "image 49/1000 d:\\Desktop\\\\export_oi\\images\\test\\050631743357d5f0.jpg: 640x608 1 Coffee cup, 476.3ms\n",
      "image 50/1000 d:\\Desktop\\\\export_oi\\images\\test\\051a95013a8ca693.jpg: 512x640 2 Mobile phones, 399.6ms\n",
      "image 51/1000 d:\\Desktop\\\\export_oi\\images\\test\\0530428836bd4956.jpg: 416x640 19 Books, 309.6ms\n",
      "image 52/1000 d:\\Desktop\\\\export_oi\\images\\test\\056dae1b6833a57e.jpg: 640x640 1 Bottle, 407.1ms\n",
      "image 53/1000 d:\\Desktop\\\\export_oi\\images\\test\\05b06c481952d6d2.jpg: 448x640 5 Books, 333.3ms\n",
      "image 54/1000 d:\\Desktop\\\\export_oi\\images\\test\\05ca3422aa670ef0.jpg: 640x640 3 Bottles, 536.9ms\n",
      "image 55/1000 d:\\Desktop\\\\export_oi\\images\\test\\0656563fdd704b9f.jpg: 640x640 1 Coffee cup, 488.4ms\n",
      "image 56/1000 d:\\Desktop\\\\export_oi\\images\\test\\065cfe7c1f112dbf.jpg: 640x640 1 Apple, 382.1ms\n",
      "image 57/1000 d:\\Desktop\\\\export_oi\\images\\test\\065f414dc792f85d.jpg: 480x640 14 Books, 350.3ms\n",
      "image 58/1000 d:\\Desktop\\\\export_oi\\images\\test\\067a21d43b856f7a.jpg: 640x640 1 Orange, 413.4ms\n",
      "image 59/1000 d:\\Desktop\\\\export_oi\\images\\test\\068b6cd2ceef28c0.jpg: 448x640 1 Mobile phone, 1 Coffee cup, 381.6ms\n",
      "image 60/1000 d:\\Desktop\\\\export_oi\\images\\test\\0697d514e448855d.jpg: 640x640 9 Oranges, 352.9ms\n",
      "image 61/1000 d:\\Desktop\\\\export_oi\\images\\test\\06a1131d642da3e4.jpg: 416x640 1 Mobile phone, 296.0ms\n",
      "image 62/1000 d:\\Desktop\\\\export_oi\\images\\test\\07048ea0ca4d5e4f.jpg: 640x544 1 Book, 429.4ms\n",
      "image 63/1000 d:\\Desktop\\\\export_oi\\images\\test\\071ddd9232b9c22c.jpg: 448x640 4 Bottles, 298.9ms\n",
      "image 64/1000 d:\\Desktop\\\\export_oi\\images\\test\\072d5447ccfc36b5.jpg: 448x640 3 Books, 305.8ms\n",
      "image 65/1000 d:\\Desktop\\\\export_oi\\images\\test\\0732662db082423f.jpg: 448x640 1 Mobile phone, 358.5ms\n",
      "image 66/1000 d:\\Desktop\\\\export_oi\\images\\test\\07462a0005636821.jpg: 640x448 4 Bottles, 321.9ms\n",
      "image 67/1000 d:\\Desktop\\\\export_oi\\images\\test\\07896440cd7993a3.jpg: 480x640 2 Mobile phones, 360.8ms\n",
      "image 68/1000 d:\\Desktop\\\\export_oi\\images\\test\\0790092f10116349.jpg: 640x640 2 Bottles, 423.9ms\n",
      "image 69/1000 d:\\Desktop\\\\export_oi\\images\\test\\07be078a07dda759.jpg: 448x640 10 Books, 334.4ms\n",
      "image 70/1000 d:\\Desktop\\\\export_oi\\images\\test\\07ef43face9d169b.jpg: 640x640 (no detections), 372.4ms\n",
      "image 71/1000 d:\\Desktop\\\\export_oi\\images\\test\\081b80f4199a9032.jpg: 640x640 1 Bottle, 350.1ms\n",
      "image 72/1000 d:\\Desktop\\\\export_oi\\images\\test\\084b363c71a7f958.jpg: 640x480 1 Bottle, 355.7ms\n",
      "image 73/1000 d:\\Desktop\\\\export_oi\\images\\test\\086ad91410243048.jpg: 480x640 14 Oranges, 458.7ms\n",
      "image 74/1000 d:\\Desktop\\\\export_oi\\images\\test\\087ede7d6249ed0a.jpg: 448x640 1 Coffee cup, 336.5ms\n",
      "image 75/1000 d:\\Desktop\\\\export_oi\\images\\test\\08aa142f47a36b62.jpg: 448x640 8 Bottles, 266.2ms\n",
      "image 76/1000 d:\\Desktop\\\\export_oi\\images\\test\\08c49e65222c87a2.jpg: 448x640 1 Coffee cup, 303.2ms\n",
      "image 77/1000 d:\\Desktop\\\\export_oi\\images\\test\\08d2eed131f4b5d9.jpg: 448x640 (no detections), 269.1ms\n",
      "image 78/1000 d:\\Desktop\\\\export_oi\\images\\test\\08ed88f402bc2ce1.jpg: 480x640 (no detections), 331.3ms\n",
      "image 79/1000 d:\\Desktop\\\\export_oi\\images\\test\\090dc3017fdc4219.jpg: 384x640 85 Bottles, 365.7ms\n",
      "image 80/1000 d:\\Desktop\\\\export_oi\\images\\test\\09126fdfbc0a58fd.jpg: 640x640 1 Mobile phone, 416.9ms\n",
      "image 81/1000 d:\\Desktop\\\\export_oi\\images\\test\\091fd0f4661b5d8c.jpg: 480x640 3 Mobile phones, 485.0ms\n",
      "image 82/1000 d:\\Desktop\\\\export_oi\\images\\test\\093f6863e99a8601.jpg: 480x640 2 Bottles, 324.2ms\n",
      "image 83/1000 d:\\Desktop\\\\export_oi\\images\\test\\0966f6cd9722b070.jpg: 480x640 1 Coffee cup, 310.7ms\n",
      "image 84/1000 d:\\Desktop\\\\export_oi\\images\\test\\096ca56a5c51f6d6.jpg: 448x640 1 Apple, 2 Oranges, 280.2ms\n",
      "image 85/1000 d:\\Desktop\\\\export_oi\\images\\test\\099fff17bf3f9a79.jpg: 448x640 1 Apple, 325.7ms\n",
      "image 86/1000 d:\\Desktop\\\\export_oi\\images\\test\\09abca0b779a8b9f.jpg: 480x640 1 Mobile phone, 379.6ms\n",
      "image 87/1000 d:\\Desktop\\\\export_oi\\images\\test\\09eb9f5005da8815.jpg: 448x640 5 Books, 330.0ms\n",
      "image 88/1000 d:\\Desktop\\\\export_oi\\images\\test\\0a51958fcd523ae4.jpg: 640x544 12 Books, 381.2ms\n",
      "image 89/1000 d:\\Desktop\\\\export_oi\\images\\test\\0a6b2b1f466501b4.jpg: 448x640 1 Book, 303.4ms\n",
      "image 90/1000 d:\\Desktop\\\\export_oi\\images\\test\\0a951714918ace53.jpg: 480x640 30 Books, 497.3ms\n",
      "image 91/1000 d:\\Desktop\\\\export_oi\\images\\test\\0aec86633421382f.jpg: 480x640 1 Coffee cup, 352.0ms\n",
      "image 92/1000 d:\\Desktop\\\\export_oi\\images\\test\\0b0803480232cf3e.jpg: 480x640 6 Bottles, 406.1ms\n",
      "image 93/1000 d:\\Desktop\\\\export_oi\\images\\test\\0b180712901c4187.jpg: 448x640 1 Orange, 312.4ms\n",
      "image 94/1000 d:\\Desktop\\\\export_oi\\images\\test\\0b2e303697ae3875.jpg: 640x480 1 Mobile phone, 403.9ms\n",
      "image 95/1000 d:\\Desktop\\\\export_oi\\images\\test\\0b2e4b673751320f.jpg: 480x640 1 Banana, 360.1ms\n",
      "image 96/1000 d:\\Desktop\\\\export_oi\\images\\test\\0b6008127f1034f7.jpg: 640x640 1 Coffee cup, 406.3ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimgs_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Desktop\\工程实训\\GarbageDetector.py:87\u001b[0m, in \u001b[0;36mGarbageDetector.detect\u001b[1;34m(self, image_input)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m检测垃圾\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03mimage_input: 可以是文件路径、PIL图像、numpy数组等\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# 完全不需要手动预处理！\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_results(results)\n",
      "File \u001b[1;32md:\\Application\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\model.py:177\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    151\u001b[0m     source: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    152\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    This method simplifies the process of making predictions by allowing the model instance to be called directly\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Application\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\model.py:535\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Application\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:225\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Application\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_contextlib.py:57\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 57\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32md:\\Application\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:324\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 324\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[1;32md:\\Application\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:162\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    160\u001b[0m not_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_tensor:\n\u001b[1;32m--> 162\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    164\u001b[0m         im \u001b[38;5;241m=\u001b[39m im[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# BGR to RGB\u001b[39;00m\n",
      "File \u001b[1;32md:\\Application\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:201\u001b[0m, in \u001b[0;36mBasePredictor.pre_transform\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m    193\u001b[0m same_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m({x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m im}) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    194\u001b[0m letterbox \u001b[38;5;241m=\u001b[39m LetterBox(\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz,\n\u001b[0;32m    196\u001b[0m     auto\u001b[38;5;241m=\u001b[39msame_shapes\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    200\u001b[0m )\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mletterbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m im]\n",
      "File \u001b[1;32md:\\Application\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\ultralytics\\data\\augment.py:1619\u001b[0m, in \u001b[0;36mLetterBox.__call__\u001b[1;34m(self, labels, image)\u001b[0m\n\u001b[0;32m   1616\u001b[0m     dh \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m new_unpad:  \u001b[38;5;66;03m# resize\u001b[39;00m\n\u001b[1;32m-> 1619\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_unpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1621\u001b[0m         img \u001b[38;5;241m=\u001b[39m img[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = detector.detect(image_input=test_config['imgs_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detection \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m找到 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 置信度: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "for detection in results:\n",
    "    print(f\"找到 {detection['class']}, 置信度: {detection['confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
